# -*- coding: utf-8 -*-
"""AoL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1llMbtEcGTM0-v0wZ3iM5KI5mNxeOUDGj

#Eksplorasi dan Preprocessing Data
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

# Load dataset
data = pd.read_csv('/content/diabetes_012_health_indicators_BRFSS2015.csv')

# Take a subset of the first 50 rows
data_subset = data.iloc[:50]

# Handle missing values if any
data_subset = data_subset.dropna()

# Split data into features and target
X = data_subset.drop('Diabetes_012', axis=1)
y = data_subset['Diabetes_012']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""#Implementasi Algoritma KNN"""

# Define KNN model
knn = KNeighborsClassifier()

# Hyperparameter tuning for KNN with reduced range
param_grid_knn = {'n_neighbors': range(1, 6)}
grid_knn = GridSearchCV(knn, param_grid_knn, cv=3, scoring='accuracy')
grid_knn.fit(X_train_scaled, y_train)

# Best K value
best_k = grid_knn.best_params_['n_neighbors']

# Train KNN with best K
knn = KNeighborsClassifier(n_neighbors=best_k)
knn.fit(X_train_scaled, y_train)

# Predict KNN
y_pred_knn = knn.predict(X_test_scaled)

# Evaluate KNN (Performance)
knn_accuracy = accuracy_score(y_test, y_pred_knn)
knn_precision = precision_score(y_test, y_pred_knn, average='weighted', zero_division=1)
knn_recall = recall_score(y_test, y_pred_knn, average='weighted', zero_division=1)
knn_f1 = f1_score(y_test, y_pred_knn, average='weighted', zero_division=1)

# Evaluate KNN (Accuray)
knn_accuracy = accuracy_score(y_test, y_pred_knn)
knn_error_rate = 1 - knn_accuracy

"""#Implementasi Algoritma Naive Bayes"""

# Define Naive Bayes model (Gaussian Naive Bayes)
nb = GaussianNB()

# Train Naive Bayes model
nb.fit(X_train_scaled, y_train)

# Predict Naive Bayes
y_pred_nb = nb.predict(X_test_scaled)

# Evaluate Naive Bayes (Performance)
nb_accuracy = accuracy_score(y_test, y_pred_nb)
nb_precision = precision_score(y_test, y_pred_nb, average='weighted', zero_division=1)
nb_recall = recall_score(y_test, y_pred_nb, average='weighted', zero_division=1)
nb_f1 = f1_score(y_test, y_pred_nb, average='weighted', zero_division=1)

# Evaluate Naive Bayes (Accuracy)
nb_accuracy = accuracy_score(y_test, y_pred_nb)
nb_error_rate = 1 - nb_accuracy

"""#Perbandingan Hasil"""

# Print the evaluation metrics
print("KNN Evaluation Metrics:")
print(f"Accuracy: {knn_accuracy:.4f}")
print(f"Precision: {knn_precision:.4f}")
print(f"Recall: {knn_recall:.4f}")
print(f"F1-Score: {knn_f1:.4f}")
print("\nClassification Report (KNN):\n", classification_report(y_test, y_pred_knn))
print("Confusion Matrix (KNN):\n", confusion_matrix(y_test, y_pred_knn))

print("\nNaive Bayes Evaluation Metrics:")
print(f"Accuracy: {nb_accuracy:.4f}")
print(f"Precision: {nb_precision:.4f}")
print(f"Recall: {nb_recall:.4f}")
print(f"F1-Score: {nb_f1:.4f}")
print("\nClassification Report (Naive Bayes):\n", classification_report(y_test, y_pred_nb))
print("Confusion Matrix (Naive Bayes):\n", confusion_matrix(y_test, y_pred_nb))

"""#Visualisasi Perbandingan Performa"""

# Visual comparison
metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
knn_scores = [knn_accuracy, knn_precision, knn_recall, knn_f1]
nb_scores = [nb_accuracy, nb_precision, nb_recall, nb_f1]

x = range(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(x, knn_scores, width, label='KNN', align='center')
ax.bar([i + width for i in x], nb_scores, width, label='Naive Bayes', align='center')

ax.set_xlabel('Metrics')
ax.set_ylabel('Scores')
ax.set_title('Comparison of KNN and Naive Bayes Performance')
ax.set_xticks([i + width / 2 for i in x])
ax.set_xticklabels(metrics)
ax.legend()

plt.show()

"""#Konversi Persentase"""

# Convert to percentage
knn_accuracy_percent = knn_accuracy * 100
knn_error_rate_percent = knn_error_rate * 100
nb_accuracy_percent = nb_accuracy * 100
nb_error_rate_percent = nb_error_rate * 100

"""#Visualisasi Perbandingan Akurasi"""

# Plotting
methods = ['KNN', 'Naive Bayes']
accuracy_scores = [knn_accuracy_percent, nb_accuracy_percent]
error_rates = [knn_error_rate_percent, nb_error_rate_percent]

x = np.arange(len(methods))
width = 0.35

fig, ax1 = plt.subplots(figsize=(10, 6))

# Plot accuracy bars
bars1 = ax1.bar(x - width/2, accuracy_scores, width, label='Accuracy', color='b')

# Plot error rate bars
bars2 = ax1.bar(x + width/2, error_rates, width, label='Error Rate', color='r')

# Adding labels, title, and customizing axes
ax1.set_ylabel('Percentage')
ax1.set_title('Comparison of Accuracy and Error Rate between KNN and Naive Bayes')
ax1.set_xticks(x)
ax1.set_xticklabels(methods)
ax1.legend()

# Adding percentage labels to the bars
def autolabel(bars):
    for bar in bars:
        height = bar.get_height()
        ax1.annotate(f'{height:.2f}%', xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 3),  # 3 points vertical offset
                     textcoords="offset points",
                     ha='center', va='bottom')

autolabel(bars1)
autolabel(bars2)

plt.tight_layout()
plt.show()